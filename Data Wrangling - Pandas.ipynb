{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1334cd8d",
   "metadata": {
    "id": "1334cd8d"
   },
   "source": [
    "pandas: https://pandas.pydata.org/docs/user_guide/index.html#user-guide\n",
    "\n",
    "pandas DF: https://pandas.pydata.org/docs/reference/frame.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b98ede6",
   "metadata": {
    "id": "9b98ede6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d6596a",
   "metadata": {},
   "source": [
    "<a id=\"0\"></a> <br>\n",
    " # Table of Contents  \n",
    "1. [Options](#options)\n",
    "1. [Index](#index)\n",
    "    1. [MultiIndex](#multiindex)\n",
    "1. [String](#string)\n",
    "    1. [Series.str](#series.str)\n",
    "1. [Dates](#dates)\n",
    "    1. [pd.to_datetime()](#pd.to_datetime())\n",
    "    1. [pd.date_range()](#pd.date_range())\n",
    "    1. [Series.dt](#series.dt)\n",
    "    1. [Time Delta](#timedelta)\n",
    "1. [Create DataFrame](#createdataframe)\n",
    "1. [Mutability](#mutability)\n",
    "1. [Data Types](#datatypes)\n",
    "    1. [Category](#category)\n",
    "1. [Input/Output](#input/output)\n",
    "1. [Sorting](#sorting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9795434f",
   "metadata": {
    "id": "9795434f"
   },
   "source": [
    "<a id=\"options\"></a> \n",
    "# ---------- Options ----------\n",
    "\n",
    "[Table of Contents](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f99b545",
   "metadata": {
    "id": "1f99b545"
   },
   "outputs": [],
   "source": [
    "## describe all options\n",
    "pd.describe_option()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09c1238",
   "metadata": {
    "id": "a09c1238"
   },
   "outputs": [],
   "source": [
    "## show the current value of an option\n",
    "pd.options.display.max_rows\n",
    "# .max_rows(None) .max_columns(None) .precision(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41190c29",
   "metadata": {
    "id": "41190c29"
   },
   "outputs": [],
   "source": [
    "## set an option\n",
    "pd.set_option(\"display.max_rows\", 999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807b63b6",
   "metadata": {},
   "source": [
    "<a id=\"index\"></a> \n",
    "# ---------- Index ----------\n",
    "\n",
    "[Table of Contents](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ef5a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert single/multi index to series\n",
    "index.to_series(name=\"name\", index=[1,2,3])\n",
    "\n",
    "## repeat index in order\n",
    "index.repeat(repeats=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4122440",
   "metadata": {},
   "source": [
    "<a id=\"multiindex\"><a/>\n",
    "## MultiIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acf7bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## the following 3 return the same MultiIndex\n",
    "\n",
    "## from arrays\n",
    "pd.MultiIndex.from_arrays([['a', 'a', 'b', 'b'], ['1', '2', '1', '2']],\n",
    "                          names=[\"level1\", \"level2\"])\n",
    "\n",
    "## from tuples\n",
    "pd.MultiIndex.from_tuples([('a', '1'), ('a', '2'), ('b', '1'), ('b', '2')],\n",
    "                          names=[\"level1\", \"level2\"])\n",
    "\n",
    "## cartesian product\n",
    "pd.MultiIndex.from_product([['a', 'b'], ['1', '2']],\n",
    "                           names=[\"level1\", \"level2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad04ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## slice multi-index (must specify value at all levels)\n",
    "df.loc[('a', '1')]\n",
    "\n",
    "## slicing based on a subset of levels\n",
    "df.xs(('value1', 'value2'), level=['level1', 'level3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72af7754",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get level values\n",
    "## specify integer or level name\n",
    "index.get_level_values(level=0)\n",
    "\n",
    "## flatten index\n",
    "index.to_flat_index()\n",
    "\n",
    "## drop a level of index from dataframe\n",
    "## specify integer or level name\n",
    "df.droplevel(level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5091cb00",
   "metadata": {
    "id": "5091cb00"
   },
   "source": [
    "<a id=\"string\"></a> \n",
    "# ---------- String ----------\n",
    "[Table of Contents](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c32d682",
   "metadata": {
    "id": "6c32d682"
   },
   "source": [
    "<a id=\"series.str\"></a> \n",
    "## Series.str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54e0dbb",
   "metadata": {
    "id": "c54e0dbb",
    "outputId": "c943cb38-2ce1-4151-991b-19fa0101524a"
   },
   "outputs": [],
   "source": [
    "## retrieve i-th character\n",
    "series.str[i]\n",
    "\n",
    "## replace pattern\n",
    "series.str.replace(pat=' ', repl='-', regex=False)\n",
    "\n",
    "## split string by pattern, new column for each item (up to n columns)\n",
    "series.str.split(pat='', expand=True, n=-1)\n",
    "\n",
    "## extract named groups, creating one column per group\n",
    "string_series.str.extract(r\"(?P<letter>[ab])(?P<digit>\\d)\")\n",
    "\n",
    "## concatenate series into one string value\n",
    "series.str.cat(sep=',', na_rep='-')\n",
    "\n",
    "## concatenate with another equal-length series\n",
    "series.str.cat(series2, sep='_', na_rep='')\n",
    "\n",
    "## concatenate with a equal-length dataframe, joining using index\n",
    "series.str.cat(df, sep='_', na_rep='', join='outer')\n",
    "\n",
    "# .lower() .upper() .len() .strip() .lstrip() .rstrip() \n",
    "# .repeat() .pad()\n",
    "# .isalpha() .isdigit() .islower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06452386",
   "metadata": {
    "id": "06452386"
   },
   "source": [
    "<a id=\"dates\"></a>\n",
    "# ---------- Dates ----------\n",
    "\n",
    "[Table of Contents](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b04d65",
   "metadata": {
    "id": "34b04d65"
   },
   "source": [
    "<a id=\"pd.to_datetime()\"></a> \n",
    "## pd.to_datetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c7cc67",
   "metadata": {
    "id": "b3c7cc67",
    "outputId": "adcd3b82-9068-44a8-c4ed-ee1763ec38dd"
   },
   "outputs": [],
   "source": [
    "## returns Timestamp object for 'now'\n",
    "pd.to_datetime('today')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0766d54",
   "metadata": {
    "id": "d0766d54",
    "outputId": "8d8dab78-5716-4dce-ddc0-944aac32e0bd"
   },
   "outputs": [],
   "source": [
    "## format list of string dates as DatetimeIndex, each item a Timestamp object\n",
    "pd.to_datetime(['26-03-1997', '20-04-1998'], format=\"%d-%m-%Y\")\n",
    "\n",
    "# %a - abbreviated weekday name | %A - full weekday name\n",
    "# %d - day of month zero padded\n",
    "# %b - abbreviated month name | %B - full month name\n",
    "# %m - month zero padded\n",
    "# %y - year without century zero padded | %Y - year with centure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b832031",
   "metadata": {
    "id": "8b832031"
   },
   "source": [
    "<a id=\"pd.date_range()\"></a> \n",
    "## pd.date_range()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c154a878",
   "metadata": {
    "id": "c154a878",
    "outputId": "f18e5bfe-4e77-4da3-d645-40fedcbce7c2"
   },
   "outputs": [],
   "source": [
    "## create a range of dates, at fixed intervals\n",
    "## specify start, period, freq\n",
    "## each item a Timestamp object\n",
    "pd.date_range(start='2020-01-01', periods=4, freq='M')\n",
    "\n",
    "## specify start, end, freq\n",
    "pd.date_range(start='2020-01-01', end='2020-12-01', freq='MS')\n",
    "# freq: \"4H\", \"6H\", \"D\", \"W-MON\", \"W-SUN\", \"MS\", \"M\", \"QS\", \"Q\", \"A-JAN\", \"A-DEC\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef76b56",
   "metadata": {
    "id": "6ef76b56"
   },
   "source": [
    "<a id=\"series.dt\"></a> \n",
    "## Series.dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8621a6f2",
   "metadata": {
    "id": "8621a6f2",
    "outputId": "7d170f6c-46a6-4489-87eb-6b466b4bc609"
   },
   "outputs": [],
   "source": [
    "## get component of datetime\n",
    "series.dt.month_name()\n",
    "# second, minute, hour,\n",
    "# day, dayofweek, dayofyear, day_name(), week, weekofyear, month, month_name(), quarter, year\n",
    "\n",
    "## maps to period\n",
    "series.dt.to_period('Q')\n",
    "# freq: \"4H\", \"6H\", \"D\", \"W-MON\", \"W-SUN\", \"MS\", \"M\", \"QS\", \"Q\", \"A-JAN\", \"A-DEC\"\n",
    "\n",
    "## change date format\n",
    "series.dt.strftime('%d/%m/%Y')\n",
    "\n",
    "## make series time zone aware\n",
    "series.dt.tz_localize('EST')\n",
    "\n",
    "## change time zone aware series to another time zone\n",
    "series.dt.tz_convert('US/Pacific')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e98ec3c",
   "metadata": {
    "id": "7e98ec3c"
   },
   "source": [
    "<a id=\"timedelta\"></a>\n",
    "## Time Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27df3fa",
   "metadata": {
    "id": "c27df3fa",
    "outputId": "22a6fe5f-2ff6-42f6-fd07-efce5c8c1d5d"
   },
   "outputs": [],
   "source": [
    "## difference between two Timestamps\n",
    "## returns Timedelta object\n",
    "time_delta = pd.Timestamp(\"2020/03/24 00:18:48\") - pd.to_datetime(\"2019/03/04 00:12:23\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b2dca0",
   "metadata": {
    "id": "40b2dca0",
    "outputId": "a338a985-0e04-41f1-e0d6-531cff037abf"
   },
   "outputs": [],
   "source": [
    "## extract time component from Timedelta\n",
    "time_delta.days \n",
    "# microseeconds, seconds, total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7a0c6d",
   "metadata": {
    "id": "af7a0c6d",
    "outputId": "fe16c3c3-5d1a-48c4-fa79-e7ea85f8f5ea"
   },
   "outputs": [],
   "source": [
    "## subtract Timestamp by Timedelta\n",
    "## absolute time arithmetic\n",
    "pd.to_datetime(\"2020/03/24 00:18:48\") - pd.Timedelta(days=9, weeks=1)\n",
    "# kwargs: years, months, weeks, days, hours, minutes\n",
    "#         seconds, milliseconds, microseconds, nanoseconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf6decc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## subtract Timestamp by DateOffset\n",
    "## respects calendar arithmetic\n",
    "pd.to_datetime(\"2020/03/24 00:18:48\") - pd.DateOffset(days=9, weeks=1)\n",
    "# kwargs: years, months, weeks, days, hours, minutes\n",
    "#         seconds, milliseconds, microseconds, nanoseconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e83035",
   "metadata": {
    "id": "58e83035"
   },
   "source": [
    "<a id=\"createdataframe\"><a/>\n",
    "# ---------- Create DataFrame ----------\n",
    "    \n",
    "[Table of Contents](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e1c06c",
   "metadata": {
    "id": "78e1c06c"
   },
   "outputs": [],
   "source": [
    "## from dictionary\n",
    "pd.DataFrame({'col1': [10,20], 'col2': ['a','b']}, index=[1,2])\n",
    "\n",
    "## from nested list\n",
    "pd.DataFrame([[1,2,3], ['a','b','c']], index=[1,2], columns=['col1', 'col2', 'col3'])\n",
    "\n",
    "## from list of dictionaries\n",
    "pd.DataFrame([{'col1': 1, 'col2': 2}, {'col1': 1, 'col3': 3}], index=[1,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8498b4",
   "metadata": {},
   "source": [
    "<a id=\"mutability\"><a/>\n",
    "# ---------- Mutability ----------\n",
    "\n",
    "[Table of Contents](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e810163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## changing cell (0,2) will not change all values in column 2\n",
    "## although list multiplication created 10 references to the same list\n",
    "df1 = pd.DataFrame([[0,1,2]]*10, columns=['a','b','c'])\n",
    "df1.iloc[0,2] = 2000\n",
    "\n",
    "## changes the original df, unless copied\n",
    "def func(df):\n",
    "    df = df.copy(deep=True)\n",
    "    df.loc[5,0] = 1000\n",
    "    \n",
    "## df can be modified through df2\n",
    "## i.e. a view is created using .iloc, .loc, or df[\"col\"]\n",
    "df2 = df1.iloc[4:8,0:2].T\n",
    "df2.iloc[1,2] = 1000\n",
    "\n",
    "## a copy is created\n",
    "df2 = df1.loc[df1.index.repeat(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af60a43c",
   "metadata": {},
   "source": [
    "<a id=\"datatypes\"><a/>\n",
    "# ---------- Data Types ----------\n",
    "\n",
    "[Table of Contents](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86e7d71",
   "metadata": {
    "id": "f86e7d71"
   },
   "outputs": [],
   "source": [
    "## change type for entire dataframe\n",
    "df.astype(int)\n",
    "\n",
    "## change type for particular columns\n",
    "df.astype({'colname1': 'float32', 'colname2': 'str'})\n",
    "\n",
    "## convert to numeric\n",
    "pd.to_numeric(series, errors='coerce')\n",
    "# errors: 'ignore', 'raise', 'coerce'\n",
    "\n",
    "## convert to numpy array / dataframe\n",
    "series.to_numpy() \n",
    "# .to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8e351f",
   "metadata": {
    "id": "eb8e351f",
    "outputId": "b32e5962-1ec1-44fc-8503-af1ec0392b65"
   },
   "outputs": [],
   "source": [
    "## .astype('string') changes np.nan to pd.NA\n",
    "pd.DataFrame([[1.0, 1, pd.NA, np.nan]]).astype('string')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8471449",
   "metadata": {
    "id": "a8471449"
   },
   "source": [
    "<a id=\"category\"><a/>\n",
    "## Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec211ca",
   "metadata": {
    "id": "cec211ca",
    "outputId": "1ba51104-65d2-4b3b-ff1f-f4ff87348c93"
   },
   "outputs": [],
   "source": [
    "## convert values to 'category' type\n",
    "series.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae4de13",
   "metadata": {
    "id": "bae4de13",
    "outputId": "ac2619ba-58c6-40ec-9de6-087f48aa4b52"
   },
   "outputs": [],
   "source": [
    "## rename categories\n",
    "## a list, or a dict, or a callable\n",
    "series.cat.rename_categories(['excellent', 'good', 'bad'])\n",
    "\n",
    "## set order of categories\n",
    "series.cat.set_categories(['bad', 'good', 'great'], ordered=True)\n",
    "\n",
    "## reorder\n",
    "series.cat.reorder_categories(['great', 'good', 'bad'], ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed59182e",
   "metadata": {
    "id": "ed59182e"
   },
   "outputs": [],
   "source": [
    "# .remove_unused_categories()\n",
    "# .remove_categories(['cat1', 'cat2'])\n",
    "# .as_ordered()\n",
    "# .as_unordered()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f32f8d9",
   "metadata": {
    "id": "1f32f8d9"
   },
   "source": [
    "<a id=\"input/output\"><a/>\n",
    "# ---------- Input / Output ----------\n",
    "\n",
    "[Table of Contents](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5256a5a2",
   "metadata": {
    "id": "5256a5a2"
   },
   "outputs": [],
   "source": [
    "path = (\"C:/Users/mushj/Desktop/WORK/\")\n",
    "\n",
    "## read table\n",
    "data = pd.read_table(path + \"filename.csv\",\n",
    "                     header=3, sep=',', nrows=-1)\n",
    "\n",
    "## .xlsx files\n",
    "data = pd.read_excel(path + \"filename.xlsx\",\n",
    "                     header=3, sheet_name='sheet_name or #')\n",
    "# sheet_name=None (read all sheets)\n",
    "\n",
    "## .csv files\n",
    "data = pd.read_csv(path + \"filename.csv\",\n",
    "                   sep=';', delim_whitespace=False)\n",
    "\n",
    "## writing\n",
    "data.to_csv(\"filename.csv\", index=False)\n",
    "data.to_excel(\"filename.xlsx\")\n",
    "data.to_sql(\"filename.sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31df188",
   "metadata": {},
   "outputs": [],
   "source": [
    "## writing to multiple sheets in Excel workbook\n",
    "with pd.ExcelWriter(\"filename.xlsx\",\n",
    "                    date_format=\"YYYY-MM-DD\",\n",
    "                    mode=\"a\", # 'w', 'a'\n",
    "                    engine=\"openpyxl\",\n",
    "                    if_sheet_exists=\"replace\" # ‘error’, ‘new’, ‘replace’, ‘overlay’\n",
    "                   ) as writer:\n",
    "    df1.to_excel(writer, sheet_name=\"Sheet1\")\n",
    "    df2.to_excel(writer, sheet_name=\"Sheet2\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f93438b",
   "metadata": {
    "id": "3f93438b"
   },
   "source": [
    "# ---------- Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0879160",
   "metadata": {
    "id": "d0879160"
   },
   "outputs": [],
   "source": [
    "data.shape\n",
    "data.columns.values\n",
    "data.index.values\n",
    "data.dtypes\n",
    "data.head(n=5)\n",
    "data.tail(n=5)\n",
    "data.info()\n",
    "\n",
    "# use .equals to compare df with np.nan, as np.nan != np.nan\n",
    "(data * 2).equals(data + data)\n",
    "\n",
    "# descriptive overview with custom percentiles\n",
    "data.describe(percentiles=[0.05, 0.1, 0.25], include=['float64'])\n",
    "\n",
    "## get unique values / counts of unique values\n",
    "df|series.nunique()\n",
    "series.unique()\n",
    "\n",
    "data.duplicated(subset=['colname1', 'colname2'], keep=False) # keep: first, last\n",
    "\n",
    "# count value by group\n",
    "# add .to_frame(name='colname') to format as DataFrame\n",
    "data.value_counts([\"colname1\", \"colname2\", \"colname3\"],\n",
    "                  normalize=False sort=True, ascending=False)\n",
    "\n",
    "# print value counts for all object variables\n",
    "for col in data.columns.values:\n",
    "    if data[col].dtypes == 'object':\n",
    "        print(data.value_counts(col))\n",
    "        print('\\n')\n",
    "\n",
    "# check NaNs\n",
    "# axis=0 -> sum across rows, for each column\n",
    "data.isnull().sum(axis=0)\n",
    "data.notnull().sum(axis=0)\n",
    "data.isna().sum(axis=0)\n",
    "data.notna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dd5097",
   "metadata": {
    "id": "56dd5097"
   },
   "source": [
    "# ---------- Math/Stat Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ead641f",
   "metadata": {
    "id": "8ead641f"
   },
   "outputs": [],
   "source": [
    "# --- descriptive statistics\n",
    "# Dataframe & Series:\n",
    "# .count() .sum() .mean() .mad() .median() .min() .max() .mode()\n",
    "# .abs() .prod() .std() .var() .sem() .skew() .kurt() .quantile(0.75)\n",
    "# .cumsum() .cumprod() .cummin() .cummax()\n",
    "# .pct_change(periods=1)\n",
    "# data['colname1'].cov(data['colname2'])  data.cov()  | .corr(method='')\n",
    "\n",
    "# standard error of mean\n",
    "data.sem(axis=0, numeric_only=True, skipna=True)\n",
    "\n",
    "# Series:\n",
    "# .autocorr(lag=1)\n",
    "data['colname'].autocorr(lag=5)\n",
    "\n",
    "# --- arithmetic\n",
    "# .add() .sub() .mul() .div() .floordiv() .mod() .pow()\n",
    "data.sub([1, 2, 3], axis=1, fill_value=None) # broadcast rowwise\n",
    "\n",
    "# Dataframe & Series\n",
    "# closed: left, right, both, neither\n",
    "data.rolling(window=4, center=False, closed=None).mean()\n",
    "# rolling with custom function: Mean Absolute Deviation from mean within window\n",
    "data.rolling(window=4).apply(lambda x: np.abs(x - x.mean()).mean())\n",
    "\n",
    "# get index of min/max\n",
    "# .idxmin()\n",
    "data.idxmax(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e68f146",
   "metadata": {
    "id": "7e68f146"
   },
   "outputs": [],
   "source": [
    "# correlation matrix: get corrs > 0.7\n",
    "cor_mat = data.corr()\n",
    "pairs = cor_mat.unstack().sort_values(ascending=False)\n",
    "pairs_cleaned = pairs[[i[0] != i[1] for i in pairs.index]].drop_duplicates() # remove self-corr and symmetry\n",
    "pairs_cleaned[pairs_cleaned > 0.7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6ce1b5",
   "metadata": {
    "id": "3b6ce1b5"
   },
   "source": [
    "# ---------- Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3d7e7e",
   "metadata": {
    "id": "3b3d7e7e"
   },
   "outputs": [],
   "source": [
    "# rename columns\n",
    "data.columns = ['newcolname1', 'newcolname2', 'newcolname3']\n",
    "\n",
    "# rename selected columns\n",
    "data.rename(columns={'oldcolname1': 'newcolname1', 'oldcolname2': 'newcolname2'},\n",
    "            index={'oldrowname': 'newrowname'})\n",
    "\n",
    "# add new index\n",
    "new_ind = data.index.union(pd.Index([3,4,5,6]))\n",
    "data.reindex(new_ind)\n",
    "\n",
    "# reset index; drop=True -> don't keep index as column\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# set a certain variable as index\n",
    "data.set_index(keys='varname', drop=True)\n",
    "\n",
    "# label axis names with list object, axis=0 (row names), axis=1 (colnames)\n",
    "data.set_axis(labels=['a', 'b', 'c'], axis=1)\n",
    "\n",
    "# name series before concatenating to dataframe\n",
    "pd.Series(new_col, name='new_colname')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd846126",
   "metadata": {
    "id": "dd846126"
   },
   "source": [
    "<a id=\"sorting\"><a/>\n",
    "# ---------- Sorting ----------\n",
    "[Table of Contents](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce490e2c",
   "metadata": {
    "id": "ce490e2c"
   },
   "outputs": [],
   "source": [
    "## reorder dataframe based on new index/column order\n",
    "df.reindex([4, 2, 1, 3], axis=0)\n",
    "df.reindex(index=[4, 2, 1, 3], columns=['C', 'A', 'B'])\n",
    "\n",
    "## if there are new indexes, fill with\n",
    "## ffill (forward), bfill (backward), nearest\n",
    "df.reindex(index=[1,2,3,4,5], method='ffill')\n",
    "\n",
    "## sort data by values of a column(s)\n",
    "df.sort_values(['colname1', 'colname2'], ascending=False, axis=0)\n",
    "\n",
    "## sort data by a level of multiindex\n",
    "df.sort_index(level=[1,0], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30d3899",
   "metadata": {
    "id": "e30d3899"
   },
   "source": [
    "<a id=\"merging\"><a/>\n",
    "# ---------- Merging ----------\n",
    "[Table of Contents](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e69829",
   "metadata": {
    "id": "38e69829"
   },
   "outputs": [],
   "source": [
    "## concatenate two dataframes / series row-wise or column-wise\n",
    "pd.concat(objs=[df1, df2, df3], axis=1, ignore_index=True, join='outer')\n",
    "\n",
    "# conventional merging\n",
    "pd.merge(left=left_df, right=right_df, on='join_column', how='left/right/outer/inner/cross',\n",
    "         suffixes=['_left', '_right'])\n",
    "\n",
    "# joining from map\n",
    "dct = {'Male': 1, 'Female': 2}\n",
    "df['tip_means_by_sex'] = df['sex'].map(dct)\n",
    "\n",
    "# patch NaNs in df1 with values in df2 (df1, df2 like-indexed)\n",
    "df1.combine_first(df2)\n",
    "\n",
    "# replace non NaNs in df1 with values in df2 (df1, df2 like-indexed)\n",
    "df1.update(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2311f0b9",
   "metadata": {
    "id": "2311f0b9",
    "outputId": "27158509-a70a-402e-bdbb-a84e1a2a2473"
   },
   "outputs": [],
   "source": [
    "# left join example:\n",
    "df1 = pd.DataFrame({'col1': ['a', 'b'], 'col2': [1, 2]})\n",
    "df2 = pd.DataFrame({'col1': ['a', 'a', 'b', 'b', 'b'], 'col3': [101, 102, 901, 902, 903]})\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e51278",
   "metadata": {
    "id": "95e51278",
    "outputId": "9a672586-afce-4737-f3a5-5599be760d9d"
   },
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a5545e",
   "metadata": {
    "id": "14a5545e",
    "outputId": "40ec92f9-7e44-4232-bc50-4867ba08d87c"
   },
   "outputs": [],
   "source": [
    "df1.merge(df2, on='col1', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec25b903",
   "metadata": {
    "id": "ec25b903"
   },
   "source": [
    "# ---------- Slice / Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4a531e",
   "metadata": {
    "id": "9b4a531e"
   },
   "outputs": [],
   "source": [
    "# dtypes: int64, float64, bool, datetime64, timedelta64, object, category\n",
    "data.select_dtypes(include=['int64'], exclude=['object'])\n",
    "\n",
    "# .nsmallest\n",
    "data.nlargest(4, 'colname')\n",
    "\n",
    "# label-based: search by row/col labels\n",
    "data.loc[['2017-01-03', '2017-01-05'], ['colname1', 'colname2']]\n",
    "\n",
    "# search by index\n",
    "data.iloc[0:10, 0:2]\n",
    "\n",
    "# get iloc index of column\n",
    "data.columns.get_loc('colname')\n",
    "\n",
    "# random sample: specify <n> or <frac>\n",
    "data.sample(n=3, weights='weights_column', replace=False, axis=0, random_state=42)\n",
    "# randomize\n",
    "data.sample(frac=1).reset_index(inplace=True, drop=True)\n",
    "\n",
    "# filter data where values of colname is in a list\n",
    "data[data['colname'].isin(['A', 'B', 'C'])]\n",
    "\n",
    "# filter data where column name contains string\n",
    "data[data['colname'].str.contains('hello')]\n",
    "\n",
    "# filter by multiple conditions on rows and columns\n",
    "# add '~' to condition as negation, '|' for or\n",
    "data.loc[(data['colname1'] > 0) & ~(df_sales['colname2'] == 100), ['colname3']]\n",
    "\n",
    "\n",
    "# cut by specifying bins: [0,18], (18,40],...\n",
    "pd.cut(x=data.age,\n",
    "       bins=[0, 18, 40, 60, np.infty],\n",
    "       include_lowest=True,\n",
    "       right=True,\n",
    "       labels=['young', 'adult', 'old', 'retired'])\n",
    "\n",
    "# cut by specifying percentile bounds\n",
    "# or number of percentiles: q=10 -> deciles\n",
    "pd.qcut(x=data.age, q=[0.25, 0.5, 0.75, 1], labels=['A', 'B', 'C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413e071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## iterate through rows\n",
    "for i,row in df.iterrows():\n",
    "    print(i, row[\"col1\"], row[\"col2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9566acf6",
   "metadata": {},
   "source": [
    "## query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e220bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reference a variable\n",
    "df.query(\"col_name >= @var\")\n",
    "\n",
    "## multiple filters\n",
    "df.query(\"col_name == 'abc' & col_name == 'xyz'\")\n",
    "\n",
    "## column name with space\n",
    "df.query(\"`col name` < 1\")\n",
    "\n",
    "## operations\n",
    "df.query(\"col_name != 1 + 2\")\n",
    "\n",
    "## reference other columns\n",
    "df.query(\"col_name1 > (col_name2 + col_name3) / 2\")\n",
    "\n",
    "## .isin()\n",
    "df.query(\"`col name`.isin([1,2,3])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d09d9d",
   "metadata": {
    "id": "81d09d9d"
   },
   "source": [
    "# ---------- Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2183e6",
   "metadata": {
    "id": "0b2183e6"
   },
   "outputs": [],
   "source": [
    "## unpivot all columns, creating a new level of index\n",
    "df.stack(level=-1)\n",
    "\n",
    "## reverses stack: pivots a level of index as columns\n",
    "df.unstack(level=-1)\n",
    "\n",
    "## explodes a column of arrays into rows,\n",
    "## repeating values of other columns\n",
    "df.explode(column='colname')\n",
    "\n",
    "## unpivot\n",
    "df.melt(id_vars=[\"col1\", \"col2\"], value_vars=[\"col3\", \"col4\"],\n",
    "        var_name=\"variable\", value_name=\"value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8e4737",
   "metadata": {
    "id": "8e8e4737"
   },
   "source": [
    "## pivot / crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa779a0",
   "metadata": {
    "id": "2fa779a0"
   },
   "outputs": [],
   "source": [
    "# pivot\n",
    "# expand multiple rows under 'colname1' as columns; columns times values set of columns\n",
    "data.pivot(index='colname1', columns=['colname2'], values=['colname3', 'colname4'])\n",
    "\n",
    "# pivot table\n",
    "# values: the column to be aggregated\n",
    "data.pivot_table(index=\"colname1\", columns=\"colname2\", values=\"colname3\",\n",
    "                 aggfunc='mean', margins=True)\n",
    "\n",
    "# get proportion by each row\n",
    "pivot_table.div([row1_sum, row2_sum, row3_sum], axis=0)\n",
    "\n",
    "# return cross-tabulated counts/percentages\n",
    "pd.crosstab(data[\"colname1\"], data[\"colname2\"], normalize=True)\n",
    "# with aggfunc\n",
    "pd.crosstab(data[\"colname1\"], data[\"colname2\"],\n",
    "            margins=True, values=data[\"colname3\"], aggfunc=lambda x: np.mean(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e5f5a2",
   "metadata": {
    "id": "98e5f5a2"
   },
   "source": [
    "## group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80593b2",
   "metadata": {
    "id": "f80593b2"
   },
   "outputs": [],
   "source": [
    "# as_index=True makes groups into indexes\n",
    "grouped = data.groupby(['colname1', 'colname2'], as_index=False)\n",
    "\n",
    "grouped.get_group('group_i_name') # get the dataframe of that group, group name is categorical level\n",
    "grouped.groups.keys() # get all group names (keys) | values are indexes\n",
    "[(name, group_data) for (name, group_data) in grouped] # unpack names and groups\n",
    "\n",
    "# compute aggregations\n",
    "grouped['colname3'].mean()\n",
    "grouped['colname3'].agg(['mean', 'std'])\n",
    "\n",
    "\n",
    "# group-by with .agg()\n",
    "(db1.groupby('colname')\n",
    "    .agg({'colname1': ['first', 'last'],\n",
    "          'colname2': ['sum', 'mean', 'count'],\n",
    "          'colname3': lambda x: x.max() - x.min()  # x refers to all rows in each group\n",
    "         })\n",
    ")\n",
    "# with names\n",
    "(data.groupby('colname')\n",
    "     .agg(agg_name1 = ('colname1', 'sum'),\n",
    "          range = ('colname2', lambda x: x.max() - x.min())\n",
    "         )\n",
    ")\n",
    "\n",
    "\n",
    "# custom named function\n",
    "custom_function = lambda x: x\n",
    "custom_function.__name__ = 'func_name'\n",
    "data.agg([custom_function])\n",
    "\n",
    "\n",
    "# group by custom index values\n",
    "df.groupby(lambda x: x.year).sum()\n",
    "\n",
    "# apply transformation within each group\n",
    "data.groupby('colname1').transform(lambda x: x - x.mean())\n",
    "data.groupby('colname1').rolling(window=3)['colname2'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba7a00a",
   "metadata": {
    "id": "6ba7a00a"
   },
   "source": [
    "# ---------- Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08328e07",
   "metadata": {
    "id": "08328e07"
   },
   "source": [
    "## Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645e6339",
   "metadata": {
    "id": "645e6339"
   },
   "outputs": [],
   "source": [
    "# Dataframe & Series:\n",
    "data.shift(-1) # lead (shift backward/right)\n",
    "data.shift(1) # lag (shift forward/left)\n",
    "data.diff(1) # difference between two-elements: i - (i-1), start from i=0\n",
    "data.diff(-1) # difference between two-elements: (i-1) - i, start from i=0\n",
    "\n",
    "# create new column based on existing columns\n",
    "data[\"new colname\"] = data[\"colname1\"] + data[\"colname2\"]\n",
    "\n",
    "\n",
    "# --- handling NaN\n",
    "# drop na values by column; subset=Null for entire dataset\n",
    "data.dropna(subset=[\"colname\"], axis=0, inplace=True)\n",
    "\n",
    "# fill na\n",
    "data.fillna(value=0) # fill all NaN with value\n",
    "data.fillna(value={'colname1': 0, 'colname2': 100}) # fill with dictionary (keys must match colnames)\n",
    "data.fillna(value=data.mean()[['colname1', 'colname2']]) # fill with series (index must match colnames)\n",
    "data.fillna(method=None) # ‘bfill’, ‘ffill’\n",
    "\n",
    "# interpolation\n",
    "# linear, quadratic, cubic\n",
    "data.interpolate(method='linear')\n",
    "\n",
    "# drop columns based on name\n",
    "data.drop(columns=[\"colname1\", \"colname2\"], axis=1, inplace=True)\n",
    "\n",
    "# drop columns based on index\n",
    "data.drop(columns=data.columns[i], axis=1)\n",
    "\n",
    "# drop duplicated rows\n",
    "data.drop_duplicates(subset=[\"colname1\", \"colname2\"], keep={'first', 'last', False})\n",
    "\n",
    "\n",
    "# replace values\n",
    "data.replace(to_replace=[\"yes\", \"no\"], value=[1, 0], inplace=False)\n",
    "\n",
    "# replace with dictionary\n",
    "data.replace({'oldvalue1': 'newvalue1', 'oldvalue2': 'newvalue2'})\n",
    "\n",
    "# replace values with regex\n",
    "data.column.replace(to_replace='^(.)|(.)$', value='_', regex=True)\n",
    "\n",
    "\n",
    "## get dummies\n",
    "pd.get_dummies(df|series, prefix=\"prefix\", prefix_sep=\"_\", drop_first=False)\n",
    "\n",
    "# returns dataframe of same shape as original, values outside of condition become NaN\n",
    "data.where(data > 0)\n",
    "# or replace those values with 'other'\n",
    "data.where(data > 0, other=-data, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5929b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "## replace multiple columns at once\n",
    "df[[\"A_new\", \"B_new\", \"C_new\"]] = df[[\"A\", \"B\", \"C\"]].values * df[\"D\"].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2465e275",
   "metadata": {},
   "outputs": [],
   "source": [
    ".resample()\n",
    ".ffill()\n",
    ".bfill()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56058625",
   "metadata": {
    "id": "56058625"
   },
   "source": [
    "## apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d730bd46",
   "metadata": {
    "id": "d730bd46"
   },
   "outputs": [],
   "source": [
    "# if axis=0, average of top and bottom values for each column\n",
    "# if axis=1, average of leftmost and rightmost values for each row\n",
    "\n",
    "# apply function to each element in data frame\n",
    "data.applymap(func=lambda x: x**2)\n",
    "\n",
    "# creating a new column based on two columns\n",
    "# axis=1: apply function to each row\n",
    "data[['Double_Header', 'Tickets_Sold']].apply(lambda x: x[1] if pd.isna(x[0]) else x[1]/2, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cf76b1",
   "metadata": {
    "id": "73cf76b1"
   },
   "source": [
    "## pipe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72533a9",
   "metadata": {
    "id": "e72533a9"
   },
   "outputs": [],
   "source": [
    "def step_1(df, na_subset):\n",
    "    return df.dropna(axis=0, subset=[na_subset])\n",
    "\n",
    "def step_2(df, operation='add', constants=[0,0,0]):\n",
    "    if operation == 'add':\n",
    "        return df.add(constants, axis=1)\n",
    "\n",
    "(data.pipe(step_1, na_subset='region A')\n",
    "     .pipe(step_2, operation='add', constants=[10, 20, 30])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f538f832",
   "metadata": {
    "id": "f538f832"
   },
   "source": [
    "## transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2b0015",
   "metadata": {
    "id": "7e2b0015"
   },
   "outputs": [],
   "source": [
    "# selective transformation\n",
    "data.transform({'colname1': np.abs,\n",
    "                'colname2': lambda x: x + 1})\n",
    "\n",
    "# apply multiple functions to one column\n",
    "data['colname'].transform([np.abs, lambda x: x + 1])\n",
    "\n",
    "# apply multiple functions to all columns\n",
    "data.transform([np.abs, lambda x: x + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33113eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby().transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2532f455",
   "metadata": {
    "id": "2532f455"
   },
   "source": [
    "## assign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49d941c",
   "metadata": {
    "id": "a49d941c"
   },
   "outputs": [],
   "source": [
    "## create two new columns (column names as arguments)\n",
    "df.assign(total=lambda x: x.sum(axis=1),\n",
    "          total_times_10=lambda x: x['total'] * 10)\n",
    "\n",
    "## create two new columns with dictionary\n",
    "df.assign(**{\"var\": lambda x: x, \"var2\": lambda x: x})"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
